# Human Detection using HOG and NN
    
This is an open source implementation of Human Detection using Histogram Oriented Gradient and a two layer perceptron neural network for detecting human in 2D color
images. The project consists of four steps:  
‚Ä¢ Converting color image to black and white    
‚Ä¢ Gradient operation (Prewitt's Operator)  
‚Ä¢ Compute HOG features  
‚Ä¢ Backpropogation using a two-layer perceptron  
  
There are 20 training images (10 positive and 10 negative) and 10 test images (5 positive and 5 negative) in .bmp format.  All images are of size 160 (Height) X 96 (Width). Using the parameters specified in the project description, you should have 20 X 12 cells and 19 X 11 blocks. The size of your final HOG descriptor should be 7,524 X 1.  
  
## How to compile and run the program
The only libraries used in this program is PIL and scipy in order to read and write the image, numpy in order to save the 0-255 value of each pixel location, and math to compute the square root. No other libraries or in built functions are required for any operation including convolution. 

## Functions
### Converting Color Image to Black and White  
Converted the color image into a greyscale image using the formula I =(0.299ùëÖ + 0.587ùê∫ + 0.114ùêµ) where R, G and B are the pixel values from the red, green
and blue channels of the color image.

### Gradient Operation (Prewitt's Operator)
The Prewitt‚Äôs operator is used for gradient operation. If part of the 3 x 3 masks of the Prewitt‚Äôs operator lies in the undefined region of the image after Gaussian filtering, output value is set to zero (indicates no edge).

### Compute HOG Features
To be implemented

L2 norm is used for block normalization. If v is the non-normalized vector containing all histograms in a given block,
![l2](data:image/svg+xml;base64,PHN2ZyB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgd2lkdGg9IjE3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%0D%0AZT4KPC9nPgo8L2c+CjwvZz4KPC9nPgo8L2c+CjwvZz4KPC9zdmc+)


Notes : Refer to lecture notes on how to compute the HOG feature. Use the unsigned representation and
quantize the computed gradient angle into one of the 9 bins as shown in Table 1 below. If the
gradient angle is in the range [170, 350) degrees, simply subtract by 180 first. Use the following
parameter values in your implementation: cell size = 8 x 8 pixels, block size = 16 x 16 pixels (or 2
x 2 cells), block overlap or step size = 8 pixels (or 1 cell.) Use L2 norm for block normalization.
Leave the histogram and final descriptor values as floating point numbers. Do not round off to
integers.



### Backpropogation using a Two-Layer Perceptron
To be implemented

###Notes
When training the neural network, you can stop when the weights do not change much between succesive epochs (or iterations) through the training set. When the weights do not change, the network outputs and hence the squared errors do not change. You can compute the mean squared error as the average of the squared errors over all 20 training samples and use it as the measure to decide when to stop training (i.e., when the mean squared error does not change much between successive epochs, you can stop).  Squared error for a traning sample is computed as E = 1 half(y - hw(x))^2 where y is the label and hw(x) is the network output.
In general, random initialization is a good strategy for initializing the weight values of a neural network.
Initialize with all 0's do not work well in general.

